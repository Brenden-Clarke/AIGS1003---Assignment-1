{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6623b9a0",
   "metadata": {},
   "source": [
    "Question 1 [10 points]\n",
    "\n",
    "1. \n",
    "P(sunny | a cone of ice cream) = P(a cone of ice cream | sunny) x P(sunny) / P(a cone of ice cream)\n",
    "\n",
    "   - Given the Naive Bayes assumption:\n",
    "   \n",
    "   P(a cone of ice cream | sunny) = [P(a | sunny) x P(cone | sunny) x P(of | sunny) x P(ice | sunny) x P(cream | sunny)]\n",
    "\n",
    "Therefore, the full expression becomes:\n",
    "\n",
    "P(sunny | a cone of ice cream) = [P(a | sunny) x P(cone | sunny) x P(of | sunny) x P(ice | sunny) x P(cream | sunny)] x P(sunny) / P(a cone of ice cream)\n",
    "\n",
    "We see a similar for the second expression: \n",
    "2. \n",
    "P(rainy | a cup of hot coffee) = P(a cup of hot coffee | rainy) x P(rainy) / P(a cup of hot coffee)\n",
    "\n",
    "   - Given the Naive Bayes assumption:\n",
    "   \n",
    "   P(a cup of hot coffee | rainy) = [P(a | rainy) x P(cup | rainy) x P(of | rainy) x P(hot | rainy) x P(coffee | rainy)]\n",
    "\n",
    "Therefore, the full expression becomes:\n",
    "\n",
    "P(rainy | a cup of hot coffee) = [P(a | rainy) x P(cup | rainy) x P(of | rainy) x P(hot | rainy) x P(coffee | rainy)] x P(rainy) / P(a cup of hot coffee)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72f0fe3",
   "metadata": {},
   "source": [
    "Question 2 [25 points]\n",
    "\n",
    "Please review the following cells of markdown, code, and outputs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dbbcca3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing classification\n",
      "--------------------\n",
      "data:\t\tdigits\n",
      "classifier:\t\tmostFrequent\n",
      "training set size:\t100\n",
      "Extracting features...\n",
      "Training...\n",
      "Validating...\n",
      "14 correct out of 100 (14.0%).\n",
      "Testing...\n",
      "14 correct out of 100 (14.0%).\n",
      "===================================\n",
      "Mistake on example 0\n",
      "Predicted 1; truth is 9\n",
      "Image: \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "             ++###+         \n",
      "             ######+        \n",
      "            +######+        \n",
      "            ##+++##+        \n",
      "           +#+  +##+        \n",
      "           +##++###+        \n",
      "           +#######+        \n",
      "           +#######+        \n",
      "            +##+###         \n",
      "              ++##+         \n",
      "              +##+          \n",
      "              ###+          \n",
      "            +###+           \n",
      "            +##+            \n",
      "           +##+             \n",
      "          +##+              \n",
      "         +##+               \n",
      "         ##+                \n",
      "        +#+                 \n",
      "        +#+                 \n",
      "                            \n"
     ]
    }
   ],
   "source": [
    "# Here we are executing the dataClassifier.py code provided with the assignment\n",
    "\n",
    "import mostFrequent\n",
    "import naiveBayes\n",
    "import samples\n",
    "import util\n",
    "\n",
    "TEST_SET_SIZE = 100\n",
    "DIGIT_DATUM_WIDTH = 28\n",
    "DIGIT_DATUM_HEIGHT = 28\n",
    "FACE_DATUM_WIDTH = 60\n",
    "FACE_DATUM_HEIGHT = 70\n",
    "\n",
    "\n",
    "def basicFeatureExtractorDigit(datum):\n",
    "    \"\"\"\n",
    "    Returns a set of pixel features indicating whether\n",
    "    each pixel in the provided datum is white (0) or gray/black (1)\n",
    "    \"\"\"\n",
    "    a = datum.getPixels()\n",
    "\n",
    "    features = util.Counter()\n",
    "    for x in range(DIGIT_DATUM_WIDTH):\n",
    "        for y in range(DIGIT_DATUM_HEIGHT):\n",
    "            if datum.getPixel(x, y) > 0:\n",
    "                features[(x, y)] = 1\n",
    "            else:\n",
    "                features[(x, y)] = 0\n",
    "    return features\n",
    "\n",
    "\n",
    "def analysis(classifier, guesses, testLabels, testData, rawTestData, printImage):\n",
    "    \"\"\"\n",
    "    This function is called after learning.\n",
    "    Include any code that you want here to help you analyze your results.\n",
    "\n",
    "    Use the printImage(<list of pixels>) function to visualize features.\n",
    "\n",
    "    An example of use has been given to you.\n",
    "\n",
    "    - classifier is the trained classifier\n",
    "    - guesses is the list of labels predicted by your classifier on the test set\n",
    "    - testLabels is the list of true labels\n",
    "    - testData is the list of training datapoints (as util.Counter of features)\n",
    "    - rawTestData is the list of training datapoints (as samples.Datum)\n",
    "    - printImage is a method to visualize the features\n",
    "    (see its use in the odds ratio part in runClassifier method)\n",
    "\n",
    "    This code won't be evaluated. It is for your own optional use\n",
    "    (and you can modify the signature if you want).\n",
    "    \"\"\"\n",
    "\n",
    "    # Put any code here...\n",
    "    # Example of use:\n",
    "    for i in range(len(guesses)):\n",
    "        prediction = guesses[i]\n",
    "        truth = testLabels[i]\n",
    "        if (prediction != truth):\n",
    "            print(\"===================================\")\n",
    "            print(\"Mistake on example %d\" % i)\n",
    "            print(\"Predicted %d; truth is %d\" % (prediction, truth))\n",
    "            print(\"Image: \")\n",
    "            print(rawTestData[i])\n",
    "            break\n",
    "\n",
    "\n",
    "## =====================\n",
    "## You don't have to modify any code below.\n",
    "## =====================\n",
    "\n",
    "\n",
    "class ImagePrinter:\n",
    "    def __init__(self, width, height):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "\n",
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.classifier = 'mostFrequent'  # Set your default classifier\n",
    "        self.data = 'digits'  # Set your default dataset\n",
    "        self.training = 100  # Set your default training set size\n",
    "        self.autotune = False  # Set autotune option as needed\n",
    "        self.iterations = 3  # Set the maximum iterations\n",
    "\n",
    "\n",
    "options = Options()\n",
    "args = {}\n",
    "\n",
    "# Set up variables according to the command line input.\n",
    "print(\"Doing classification\")\n",
    "print(\"--------------------\")\n",
    "print(\"data:\\t\\t\" + options.data)\n",
    "print(\"classifier:\\t\\t\" + options.classifier)\n",
    "print(\"training set size:\\t\" + str(options.training))\n",
    "if (options.data == \"digits\"):\n",
    "    printImage = ImagePrinter(DIGIT_DATUM_WIDTH, DIGIT_DATUM_HEIGHT)\n",
    "    featureFunction = basicFeatureExtractorDigit\n",
    "else:\n",
    "    print(\"Unknown dataset\", options.data)\n",
    "\n",
    "if (options.data == \"digits\"):\n",
    "    legalLabels = list(range(10))\n",
    "\n",
    "# Load data\n",
    "numTraining = options.training\n",
    "\n",
    "rawTrainingData = samples.loadDataFile(\"trainingimages\", numTraining, DIGIT_DATUM_WIDTH, DIGIT_DATUM_HEIGHT)\n",
    "trainingLabels = samples.loadLabelsFile(\"traininglabels\", numTraining)\n",
    "rawValidationData = samples.loadDataFile(\"validationimages\", TEST_SET_SIZE, DIGIT_DATUM_WIDTH, DIGIT_DATUM_HEIGHT)\n",
    "validationLabels = samples.loadLabelsFile(\"validationlabels\", TEST_SET_SIZE)\n",
    "rawTestData = samples.loadDataFile(\"testimages\", TEST_SET_SIZE, DIGIT_DATUM_WIDTH, DIGIT_DATUM_HEIGHT)\n",
    "testLabels = samples.loadLabelsFile(\"testlabels\", TEST_SET_SIZE)\n",
    "\n",
    "# Extract features\n",
    "print(\"Extracting features...\")\n",
    "trainingData = list(map(featureFunction, rawTrainingData))\n",
    "validationData = list(map(featureFunction, rawValidationData))\n",
    "testData = list(map(featureFunction, rawTestData))\n",
    "\n",
    "# Create classifier\n",
    "if (options.classifier == \"mostFrequent\"):\n",
    "    classifier = mostFrequent.MostFrequentClassifier(legalLabels)\n",
    "elif (options.classifier == \"naiveBayes\" or options.classifier == \"nb\"):\n",
    "    classifier = naiveBayes.NaiveBayesClassifier(legalLabels)\n",
    "    if (options.autotune):\n",
    "        print(\"using automatic tuning for naivebayes\")\n",
    "        classifier.automaticTuning = True\n",
    "else:\n",
    "    print(\"Unknown classifier:\", options.classifier)\n",
    "\n",
    "args['classifier'] = classifier\n",
    "args['featureFunction'] = featureFunction\n",
    "args['printImage'] = printImage\n",
    "\n",
    "# Conduct training and testing\n",
    "print(\"Training...\")\n",
    "classifier.train(trainingData, trainingLabels, validationData, validationLabels)\n",
    "print(\"Validating...\")\n",
    "guesses = classifier.classify(validationData)\n",
    "correct = [guesses[i] == validationLabels[i] for i in range(len(validationLabels))].count(True)\n",
    "print(str(correct), (\"correct out of \" + str(len(validationLabels)) + \" (%.1f%%).\") % (\n",
    "        100.0 * correct / len(validationLabels)))\n",
    "print(\"Testing...\")\n",
    "guesses = classifier.classify(testData)\n",
    "correct = [guesses[i] == testLabels[i] for i in range(len(testLabels))].count(True)\n",
    "print(str(correct), (\"correct out of \" + str(len(testLabels)) + \" (%.1f%%).\") % (100.0 * correct / len(testLabels)))\n",
    "analysis(classifier, guesses, testLabels, testData, rawTestData, printImage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90cdac1",
   "metadata": {},
   "source": [
    "^ Above is our output of the dataClassifier.py running the default classifier mostFrequent.py ^"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c0cd0d",
   "metadata": {},
   "source": [
    "Question 2 [25 points]\n",
    "\n",
    "Please review the following cells of markdown, code, and outputs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c8f3d709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing classification\n",
      "--------------------\n",
      "data:\t\tdigits\n",
      "classifier:\t\tnaiveBayes\n",
      "training set size:\t900\n",
      "using automatic tuning for naivebayes\n",
      "Extracting features...\n",
      "Training...\n",
      "Validating...\n",
      "60 correct out of 100 (60.0%).\n",
      "Testing...\n",
      "52 correct out of 100 (52.0%).\n",
      "===================================\n",
      "Mistake on example 0\n",
      "Predicted 7; truth is 9\n",
      "Image: \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "                            \n",
      "             ++###+         \n",
      "             ######+        \n",
      "            +######+        \n",
      "            ##+++##+        \n",
      "           +#+  +##+        \n",
      "           +##++###+        \n",
      "           +#######+        \n",
      "           +#######+        \n",
      "            +##+###         \n",
      "              ++##+         \n",
      "              +##+          \n",
      "              ###+          \n",
      "            +###+           \n",
      "            +##+            \n",
      "           +##+             \n",
      "          +##+              \n",
      "         +##+               \n",
      "         ##+                \n",
      "        +#+                 \n",
      "        +#+                 \n",
      "                            \n"
     ]
    }
   ],
   "source": [
    "# Here we are attempting to optimize the naiveBayes classifier\n",
    "\n",
    "import util\n",
    "import classificationMethod\n",
    "import math\n",
    "\n",
    "class NaiveBayesClassifier(classificationMethod.ClassificationMethod):\n",
    "    \"\"\"\n",
    "    See the project description for the specifications of the Naive Bayes classifier.\n",
    "    \n",
    "    Note that the variable 'datum' in this code refers to a counter of features\n",
    "    (not to a raw samples.Datum).\n",
    "    \"\"\"\n",
    "    def __init__(self, legalLabels):\n",
    "        self.legalLabels = legalLabels\n",
    "        self.type = \"naivebayes\"\n",
    "        self.k = 1  # this is the smoothing parameter, ** use it in your train method **\n",
    "        self.automaticTuning = True  # Look at this flag to decide whether to choose k automatically ** use this in your train method **\n",
    "\n",
    "    def setSmoothing(self, k):\n",
    "        \"\"\"\n",
    "        This is used by the main method to change the smoothing parameter before training.\n",
    "        Do not modify this method.\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "\n",
    "    def train(self, trainingData, trainingLabels, validationData, validationLabels):\n",
    " \n",
    "        \"\"\"\n",
    "        Train the Naive Bayes classifier.\n",
    "        \n",
    "        Args:\n",
    "        trainingData: A list of feature Counters for the training data.\n",
    "        trainingLabels: A list of labels for the training data.\n",
    "        validationData: A list of feature Counters for the validation data.\n",
    "        validationLabels: A list of labels for the validation data.\n",
    "        \"\"\"\n",
    "    \n",
    "        from sklearn.feature_selection import SelectKBest\n",
    "        from sklearn.feature_selection import f_classif\n",
    "\n",
    "        # Perform feature selection using SelectKBest with ANOVA F-test\n",
    "        num_features_to_select = 100  # Adjust this value as needed\n",
    "        feature_selector = SelectKBest(score_func=f_classif, k=num_features_to_select)\n",
    "        selected_training_data = feature_selector.fit_transform(trainingData, trainingLabels)\n",
    "\n",
    "        self.features = list(trainingData[0].keys())\n",
    "\n",
    "        if self.automaticTuning:\n",
    "            kgrid = [0.001, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "        else:\n",
    "            kgrid = [self.k]\n",
    "\n",
    "        bestAccuracy = -1\n",
    "        bestK = -1\n",
    "\n",
    "        for k in kgrid:\n",
    "            self.k = k\n",
    "            # Collect counts over the training data\n",
    "            self.prior = util.Counter()\n",
    "            self.condProb = {label: util.Counter() for label in self.legalLabels}\n",
    "\n",
    "            for i, label in enumerate(trainingLabels):\n",
    "                self.prior[label] += 1\n",
    "                for feature, value in trainingData[i].items():\n",
    "                    # Apply Laplace smoothing here by adding 'k' to both numerator and denominator\n",
    "                    self.condProb[label][feature, value] += k\n",
    "                    # Don't forget to increment the total count for 'label' as well\n",
    "\n",
    "            # Apply smoothing and normalization\n",
    "            for label in self.legalLabels:\n",
    "                self.prior[label] += k\n",
    "                total = self.prior[label]\n",
    "\n",
    "                for feature, value in self.features:\n",
    "                    # Normalize the probabilities here\n",
    "                    self.condProb[label][feature, value] /= total\n",
    "\n",
    "            # Classify the validation data\n",
    "            guesses = self.classify(validationData)\n",
    "            correct = [guesses[i] == validationLabels[i] for i in range(len(validationLabels))].count(True)\n",
    "            accuracy = correct / len(validationLabels)\n",
    "\n",
    "            # Update the best parameters if accuracy is better\n",
    "            if accuracy > bestAccuracy:\n",
    "                bestAccuracy = accuracy\n",
    "                bestK = k\n",
    "\n",
    "        # Set the best smoothing parameter\n",
    "        self.k = bestK\n",
    "\n",
    "        test_guesses = self.classify(testData)\n",
    "        test_correct = [test_guesses[i] == testLabels[i] for i in range(len(testLabels))].count(True)\n",
    "        test_accuracy = test_correct / len(testLabels)\n",
    "        print(\"Testing accuracy with best k (k={}): {:.2%}\".format(self.k, test_accuracy))\n",
    "    \n",
    "        # Your code for model selection or tuning goes here\n",
    "        # Any code added here won't be evaluated, it's for your own analysis\n",
    "\n",
    "    def classify(self, testData):\n",
    "        # Classify the data based on the posterior distribution over labels.\n",
    "        guesses = []\n",
    "        self.posteriors = []  # Log posteriors are stored for later data analysis (autograder).\n",
    "        for datum in testData:\n",
    "            posterior = self.calculateLogJointProbabilities(datum)\n",
    "            guesses.append(posterior.argMax())\n",
    "            self.posteriors.append(posterior)\n",
    "        return guesses\n",
    "\n",
    "    def calculateLogJointProbabilities(self, datum):\n",
    "        logJoint = util.Counter()\n",
    "        for label in self.legalLabels:\n",
    "            logJoint[label] = math.log(self.prior[label])\n",
    "            for feature, value in datum.items():\n",
    "                if (feature, value) in self.condProb[label]:\n",
    "                    logJoint[label] += math.log(self.condProb[label][feature, value])\n",
    "        return logJoint\n",
    "\n",
    "    def findHighOddsFeatures(self, label1, label2):\n",
    "        featuresOdds = []\n",
    "        odds = util.Counter()\n",
    "\n",
    "        for feature in self.features:\n",
    "            p_label1 = self.condProb[label1][feature, 1]\n",
    "            p_label2 = self.condProb[label2][feature, 1]\n",
    "\n",
    "            if p_label2 == 0:\n",
    "                odds[feature] = float('inf')\n",
    "            else:\n",
    "                odds[feature] = p_label1 / p_label2\n",
    "\n",
    "        # Find the top 100 features with the highest odds ratio\n",
    "        featuresOdds = odds.sortedKeys()[:100]\n",
    "\n",
    "        return featuresOdds\n",
    "\n",
    "    !python dataClassifier.py -c naiveBayes -a -t 900\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb2b909",
   "metadata": {},
   "source": [
    "^ Above is our output of the dataClassifier.py running the optimized naiveBayes.py ^"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
